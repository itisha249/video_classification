{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jay ganesh\n"
     ]
    }
   ],
   "source": [
    "print(\"jay ganesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_frames(video_path, frame_rate=1):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_interval = max(1, fps // frame_rate)\n",
    "    count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_interval == 0:\n",
    "            frames.append(frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "def extract_audio(video_path):\n",
    "    clip = VideoFileClip(video_path)\n",
    "    audio_path = \"audio.wav\"\n",
    "    clip.audio.write_audiofile(audio_path)\n",
    "    return audio_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        transcript = recognizer.recognize_google(audio_data)\n",
    "    return transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def audio_to_text(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            # Add the correct language code if needed\n",
    "            transcript = recognizer.recognize_google(audio_data, language=\"en-US\")\n",
    "        except sr.UnknownValueError:\n",
    "            transcript = \"[Unintelligible]\"\n",
    "        except sr.RequestError as e:\n",
    "            transcript = f\"[API error: {e}]\"\n",
    "    return transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def sound_frequency_analysis(audio_path):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr).mean()\n",
    "    # Example threshold\n",
    "    return 1 if spectral_centroid > 3000 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_wise_analysis(frames):\n",
    "    # Detect specific patterns (e.g., straight angles, zooms, pans)\n",
    "    # Example placeholder logic\n",
    "    return 0.8  # Acting-like probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_scene_detection(frames):\n",
    "    cut_count = 0\n",
    "    for i in range(1, len(frames)):\n",
    "        diff = cv2.absdiff(cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY),\n",
    "                           cv2.cvtColor(frames[i - 1], cv2.COLOR_BGR2GRAY)).mean()\n",
    "        if diff > 50:  # Threshold for scene change\n",
    "            cut_count += 1\n",
    "    return cut_count / len(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_detection(frames):\n",
    "    motion_score = 0\n",
    "    for i in range(1, len(frames)):\n",
    "        flow = cv2.calcOpticalFlowFarneback(\n",
    "            cv2.cvtColor(frames[i - 1], cv2.COLOR_BGR2GRAY),\n",
    "            cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY),\n",
    "            None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
    "        )\n",
    "        motion_score += flow.mean()\n",
    "    return motion_score / len(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_analysis(frames):\n",
    "    stability_score = 0\n",
    "    for i in range(1, len(frames)):\n",
    "        diff = cv2.absdiff(frames[i], frames[i - 1]).mean()\n",
    "        stability_score += diff\n",
    "    return 1 - (stability_score / len(frames))  # Invert for stability score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ipatel\\AppData\\Local\\anaconda3\\envs\\vclass\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def script_based_classification(transcript):\n",
    "    classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased\")\n",
    "    result = classifier(transcript)\n",
    "    return 1 if result[0]['label'] == 'POSITIVE' else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_scene(video_path):\n",
    "    frames = extract_frames(video_path)\n",
    "    audio_path = extract_audio(video_path)\n",
    "    transcript = audio_to_text(audio_path)\n",
    "\n",
    "    sound_prob = sound_frequency_analysis(audio_path)\n",
    "    angle_prob = angle_wise_analysis(frames)\n",
    "    cut_scene_prob = cut_scene_detection(frames)\n",
    "    motion_prob = motion_detection(frames)\n",
    "    stability_prob = stability_analysis(frames)\n",
    "    script_prob = script_based_classification(transcript)\n",
    "\n",
    "    # Combine probabilities\n",
    "    weights = [0.2, 0.2, 0.2, 0.2, 0.1, 0.1]  # Adjust as needed\n",
    "    probs = [sound_prob, angle_prob, cut_scene_prob, motion_prob, stability_prob, script_prob]\n",
    "    final_prob = sum(w * p for w, p in zip(weights, probs))\n",
    "\n",
    "    return \"Acting\" if final_prob > 0.5 else \"Real-Life\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'mp42', 'minor_version': '0', 'compatible_brands': 'isommp42', 'encoder': 'Google'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [512, 640], 'bitrate': 567, 'fps': 30.0, 'codec_name': 'h264', 'profile': '(Constrained Baseline)', 'metadata': {'Metadata': '', 'handler_name': 'ISO Media file produced by Google Inc.', 'vendor_id': '[0][0][0][0]'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': 'eng', 'default': True, 'fps': 48000, 'bitrate': 192, 'metadata': {'Metadata': '', 'handler_name': 'ISO Media file produced by Google Inc.', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 59.99, 'bitrate': 761, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(Constrained Baseline)', 'video_size': [512, 640], 'video_bitrate': 567, 'video_fps': 30.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 48000, 'audio_bitrate': 192, 'video_duration': 59.99, 'video_n_frames': 1799}\n",
      "c:\\Users\\ipatel\\AppData\\Local\\anaconda3\\envs\\vclass\\lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe -i D:\\video_classification\\video_classification\\fake.mp4 -loglevel error -f image2pipe -vf scale=512:640 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "MoviePy - Writing audio in audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scene is classified as: Real-Life\n"
     ]
    }
   ],
   "source": [
    "video_path = r\"D:\\video_classification\\video_classification\\fake.mp4\"\n",
    "result = classify_scene(video_path)\n",
    "print(f\"The scene is classified as: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # Optional: Check if GPU is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ipatel\\AppData\\Local\\anaconda3\\envs\\vclass\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ipatel\\AppData\\Local\\anaconda3\\envs\\vclass\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ipatel\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
